import json
import os
import numpy as np
import pandas as pd
from sentence_transformers import SentenceTransformer, models
import time
import torch

# ================= âš™ï¸ é…ç½® =================

JSON_FILE_PATH = r"D:\predict\0.1\data\2015_tree.json"
EXTERNAL_TXT_PATH = r"D:\predict\0.1\lables"
LOCAL_MODEL_PATH = r"D:\predict\models\bge-large-zh-v1.5"

# ç»“æœæ–‡ä»¶ï¼ˆåŒæ—¶ä¿å­˜ Excel å’Œ CSVï¼‰
OUTPUT_EXCEL = r"D:\predict\0.1\2015_Project_Final_Labels_GPU.xlsx"
OUTPUT_CSV = r"D:\predict\0.1\2015_Project_Final_Labels_GPU.csv"

# ä¸­é—´ç¼“å­˜æ–‡ä»¶ï¼ˆé˜²æ­¢å´©äº†ç™½è·‘ï¼‰
CACHE_EMB_PATH = r"D:\predict\0.1\2015project_embeddings_cache.npy"

BATCH_SIZE = 64


# ================= ä»£ç  =================

def load_model_on_gpu(model_path):
    print(f"\nâ¬‡ï¸  æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ–¥ï¸  è¿è¡Œè®¾å¤‡: {device}")

    word_embedding_model = models.Transformer(model_path, max_seq_length=512)
    pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), pooling_mode_cls_token=True)
    return SentenceTransformer(modules=[word_embedding_model, pooling_model], device=device)


def extract_projects(file_path):
    print(f"ğŸ“‚ è¯»å– JSON: {file_path}")
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    projects = []

    def recurse(node, path):
        name = node.get("name", "Root")
        curr_path = f"{path} > {name}" if path else name
        if "projects" in node:
            for p in node["projects"]:
                if p: projects.append({"é¡¹ç›®åç§°": p, "åŸå†…éƒ¨è·¯å¾„": curr_path})
        if "children" in node:
            for c in node["children"]:
                recurse(c, curr_path)

    if isinstance(data, dict):
        recurse(data, "")
    elif isinstance(data, list):
        for item in data: recurse(item, "")
    return pd.DataFrame(projects)


def load_external_labels(file_path):
    if not os.path.exists(file_path): file_path += ".txt"
    with open(file_path, 'r', encoding='utf-8') as f:
        return [line.strip() for line in f if line.strip()]


def main():
    print("=" * 50)
    print("ğŸš€ æœ€ç»ˆé˜²å´©æºƒç‰ˆå¯åŠ¨")
    print("=" * 50)

    # 1. åŠ è½½æ•°æ®
    df = extract_projects(JSON_FILE_PATH)
    project_names = df["é¡¹ç›®åç§°"].tolist()
    print(f"ğŸ“Š å…± {len(project_names)} æ¡é¡¹ç›®")

    # 2. æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜å‘é‡
    if os.path.exists(CACHE_EMB_PATH):
        print(f"\nâš¡ å‘ç°å·²è®¡ç®—å¥½çš„å‘é‡ç¼“å­˜: {CACHE_EMB_PATH}")
        print("â© ç›´æ¥åŠ è½½ç¼“å­˜ï¼Œè·³è¿‡æ¨¡å‹è®¡ç®—ï¼")
        proj_emb = np.load(CACHE_EMB_PATH)
        if len(proj_emb) != len(project_names):
            print("âŒ ç¼“å­˜æ•°é‡ä¸æ•°æ®ä¸ä¸€è‡´ï¼Œå°†é‡æ–°è®¡ç®—...")
            need_calc = True
        else:
            need_calc = False
            # è¿˜æ˜¯éœ€è¦åŠ è½½æ¨¡å‹æ¥ç®—ä¸€ä¸‹å¤–éƒ¨æ ‡ç­¾çš„å‘é‡
            model = load_model_on_gpu(LOCAL_MODEL_PATH)
    else:
        need_calc = True
        model = load_model_on_gpu(LOCAL_MODEL_PATH)

    # 3. è®¡ç®—é¡¹ç›®å‘é‡ (å¦‚æœæ²¡ç¼“å­˜)
    if need_calc:
        print(f"\nâš¡ å¼€å§‹è®¡ç®—é¡¹ç›®å‘é‡...")
        start_t = time.time()
        proj_emb = model.encode(project_names, normalize_embeddings=True, batch_size=BATCH_SIZE, show_progress_bar=True)
        print(f"âœ… è®¡ç®—è€—æ—¶: {time.time() - start_t:.1f}s")

        # ã€å…³é”®ã€‘ç«‹å³ä¿å­˜ç¼“å­˜
        print(f"ğŸ’¾ ä¿å­˜å‘é‡ç¼“å­˜åˆ°: {CACHE_EMB_PATH}")
        np.save(CACHE_EMB_PATH, proj_emb)

    # 4. è®¡ç®—å¤–éƒ¨æ ‡ç­¾å‘é‡
    print("\nğŸ·ï¸  è®¡ç®—å¤–éƒ¨æ ‡ç­¾å‘é‡...")
    ext_labels = load_external_labels(EXTERNAL_TXT_PATH)
    ext_emb = model.encode(ext_labels, normalize_embeddings=True, batch_size=BATCH_SIZE, show_progress_bar=False)

    # 5. åŒ¹é…
    print("\nğŸ” æ­£åœ¨åŒ¹é…...")
    sim_matrix = np.dot(proj_emb, ext_emb.T)

    results = []
    top_k = 3
    for i, row in df.iterrows():
        scores = sim_matrix[i]
        top_idx = scores.argsort()[-top_k:][::-1]
        item = row.to_dict()
        for rank, idx in enumerate(top_idx):
            item[f"å¤–éƒ¨æ ‡ç­¾_{rank + 1}"] = ext_labels[idx]
            item[f"ç›¸ä¼¼åº¦_{rank + 1}"] = round(float(scores[idx]), 4)
        results.append(item)

    # 6. ä¿å­˜ç»“æœ (åŒé‡ä¿é™©)
    df_res = pd.DataFrame(results)

    # ä¼˜å…ˆä¿å­˜ CSV (é€Ÿåº¦å¿«ï¼Œä¸ä¾èµ– openpyxl)
    print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜ CSV: {OUTPUT_CSV}")
    df_res.to_csv(OUTPUT_CSV, index=False, encoding='utf-8-sig')  # utf-8-sig é˜²æ­¢ä¸­æ–‡ä¹±ç 

    # å°è¯•ä¿å­˜ Excel
    try:
        print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜ Excel: {OUTPUT_EXCEL}")
        df_res.to_excel(OUTPUT_EXCEL, index=False)
        print("âœ… Excel ä¿å­˜æˆåŠŸ")
    except ImportError:
        print("âš ï¸ ç¼ºå°‘ openpyxl åº“ï¼ŒExcel ä¿å­˜å¤±è´¥ï¼Œä½† CSV å·²ä¿å­˜æˆåŠŸï¼")
    except Exception as e:
        print(f"âš ï¸ Excel ä¿å­˜å‡ºé”™: {e} (è¯·æŸ¥çœ‹ CSV æ–‡ä»¶)")

    print("\nğŸ‰ å…¨éƒ¨å®Œæˆï¼")


if __name__ == "__main__":
    main()